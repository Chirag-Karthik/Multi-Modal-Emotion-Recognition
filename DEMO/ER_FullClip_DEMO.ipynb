{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Emotion Recognition: audio + video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# Audio and video manipulation\n",
    "import moviepy.editor as mp\n",
    "import cv2\n",
    "import librosa\n",
    "from joblib import load\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels dictionary\n",
    "emotions_tras = {1:1, 2:4, 3:5, 4:0, 5:3, 6:2, 7:6}\n",
    "emotions = {0:'angry', 1:'calm', 2:'disgust', 3:'fear', 4:'happy', 5:'sad', 6:'surprise'}\n",
    "\n",
    "# Paths\n",
    "dataset_path = \"C:/Users/karthik chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/DEMO/Examples/\"\n",
    "haar_path = './../Other/haarcascade_frontalface_default.xml'\n",
    "parameters_path = 'C:/Users/karthik chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/Datasets/RAVDESS_audio/std_scaler.bin'\n",
    "models_video_path = \"./../Models/Video Stream/\"\n",
    "models_audio_path = \"./../Models/Audio Stream/\"\n",
    "vlc_path = \"C:/Users/ADMIN/Downloads/vlc-3.0.18-win32.exe\" # to play the selected video (insert your own path to vlc.exe)\n",
    "\n",
    "# Audio video parameters\n",
    "height_targ = 112\n",
    "width_targ = 112\n",
    "sr = 48000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "root= tk.Tk()\n",
    "\n",
    "canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text='Select clip to analize')\n",
    "label1.config(font=('helvetica', 16))\n",
    "canvas1.create_window(200, 25, window=label1)\n",
    "\n",
    "label2 = tk.Label(root, text='Number from 0 to 6:')\n",
    "label2.config(font=('helvetica', 11))\n",
    "canvas1.create_window(200, 100, window=label2)\n",
    "\n",
    "def display_text():\n",
    "   global example\n",
    "   example = int(example.get())\n",
    "   root.destroy\n",
    "\n",
    "example = tk.Entry(root)\n",
    "example.pack()\n",
    "canvas1.create_window(200, 140, window=example)\n",
    "\n",
    "    \n",
    "button1 = tk.Button(text='Select', command=lambda: [display_text(), root.destroy()], font=('helvetica', 12, 'bold'))\n",
    "canvas1.create_window(200, 180, window=button1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/karthik chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/DEMO/Examples/01-01-02-02-02-02-24.mp4Zone.Identifier\n"
     ]
    }
   ],
   "source": [
    "fn = os.listdir(dataset_path)\n",
    "filename = dataset_path + fn[example]\n",
    "print(filename)\n",
    "label = emotions_tras[int(fn[example].split('-')[2]) - 1] # trasposition of the emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a2239",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape frames: (0,)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(filename)\n",
    "haar_cascade = cv2.CascadeClassifier(haar_path)\n",
    "frames = []\n",
    "count = 0\n",
    "skip = 3\n",
    "\n",
    "# Loop through all frames\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "    if (count % skip == 0 and count > 20):\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # detect and crop face\n",
    "        faces = haar_cascade.detectMultiScale(frame, scaleFactor=1.12, minNeighbors=9)\n",
    "        if len(faces) != 1:\n",
    "            continue\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y + h, x:x + w]\n",
    "\n",
    "        face = cv2.resize(face, (height_targ+10, width_targ+10))\n",
    "        face = face[5:-5, 5:-5]\n",
    "        face = face/255.\n",
    "        frames.append(face)\n",
    "    count += 1\n",
    "\n",
    "frames = np.array(frames)\n",
    "num_frames = len(frames)\n",
    "labels = [label] * num_frames\n",
    "print('shape frames:', frames.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "MoviePy error: failed to read the duration of file C:/Users/karthik chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/DEMO/Examples/01-01-02-02-02-02-24.mp4Zone.Identifier.\nHere are the file infos returned by ffmpeg:\n\nffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\r\n  built with gcc 9.2.1 (GCC) 20200122\r\n  configuration: --enable-gpl --enable-version3 --enable-sdl2 --enable-fontconfig --enable-gnutls --enable-iconv --enable-libass --enable-libdav1d --enable-libbluray --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libtheora --enable-libtwolame --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libzimg --enable-lzma --enable-zlib --enable-gmp --enable-libvidstab --enable-libvorbis --enable-libvo-amrwbenc --enable-libmysofa --enable-libspeex --enable-libxvid --enable-libaom --enable-libmfx --enable-amf --enable-ffnvcodec --enable-cuvid --enable-d3d11va --enable-nvenc --enable-nvdec --enable-dxva2 --enable-avisynth --enable-libopenmpt\r\n  libavutil      56. 31.100 / 56. 31.100\r\n  libavcodec     58. 54.100 / 58. 54.100\r\n  libavformat    58. 29.100 / 58. 29.100\r\n  libavdevice    58.  8.100 / 58.  8.100\r\n  libavfilter     7. 57.100 /  7. 57.100\r\n  libswscale      5.  5.100 /  5.  5.100\r\n  libswresample   3.  5.100 /  3.  5.100\r\n  libpostproc    55.  5.100 / 55.  5.100\r\n[lrc @ 0000016446d89240] Format lrc detected only with low score of 5, misdetection possible!\r\nInput #0, lrc, from 'C:/Users/karthik chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/DEMO/Examples/01-01-02-02-02-02-24.mp4Zone.Identifier':\r\n  Duration: N/A, bitrate: N/A\r\n    Stream #0:0: Subtitle: text\r\nAt least one output file must be specified\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\karthik chirag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:286\u001b[0m, in \u001b[0;36mffmpeg_parse_infos\u001b[1;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[0;32m    285\u001b[0m line \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lines \u001b[39mif\u001b[39;00m keyword \u001b[39min\u001b[39;00m l][index]\n\u001b[1;32m--> 286\u001b[0m match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49mfindall(\u001b[39m\"\u001b[39;49m\u001b[39m([0-9][0-9]:[0-9][0-9]:[0-9][0-9].[0-9][0-9])\u001b[39;49m\u001b[39m\"\u001b[39;49m, line)[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m    287\u001b[0m result[\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cvsecs(match)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\karthik chirag\\Downloads\\Emotion-Recognition_SER-FER_RAVDESS\\Emotion-Recognition_SER-FER_RAVDESS-main\\DEMO\\ER_FullClip_DEMO.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/karthik%20chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/DEMO/ER_FullClip_DEMO.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m audiofile \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39;49mAudioFileClip(filename)\u001b[39m.\u001b[39mset_fps(sr)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karthik%20chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/DEMO/ER_FullClip_DEMO.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m audio \u001b[39m=\u001b[39m audiofile\u001b[39m.\u001b[39mto_soundarray()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/karthik%20chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/DEMO/ER_FullClip_DEMO.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m audio \u001b[39m=\u001b[39m audio[\u001b[39mint\u001b[39m(sr\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m):\u001b[39mint\u001b[39m(sr\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m sr\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\karthik chirag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\audio\\io\\AudioFileClip.py:70\u001b[0m, in \u001b[0;36mAudioFileClip.__init__\u001b[1;34m(self, filename, buffersize, nbytes, fps)\u001b[0m\n\u001b[0;32m     67\u001b[0m AudioClip\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m filename\n\u001b[1;32m---> 70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader \u001b[39m=\u001b[39m FFMPEG_AudioReader(filename, fps\u001b[39m=\u001b[39;49mfps, nbytes\u001b[39m=\u001b[39;49mnbytes,\n\u001b[0;32m     71\u001b[0m                                  buffersize\u001b[39m=\u001b[39;49mbuffersize)\n\u001b[0;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfps \u001b[39m=\u001b[39m fps\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mduration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader\u001b[39m.\u001b[39mduration\n",
      "File \u001b[1;32mc:\\Users\\karthik chirag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\audio\\io\\readers.py:51\u001b[0m, in \u001b[0;36mFFMPEG_AudioReader.__init__\u001b[1;34m(self, filename, buffersize, print_infos, fps, nbytes, nchannels)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39macodec \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpcm_s\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39mle\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(\u001b[39m8\u001b[39m\u001b[39m*\u001b[39mnbytes)\n\u001b[0;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnchannels \u001b[39m=\u001b[39m nchannels\n\u001b[1;32m---> 51\u001b[0m infos \u001b[39m=\u001b[39m ffmpeg_parse_infos(filename)\n\u001b[0;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mduration \u001b[39m=\u001b[39m infos[\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mvideo_duration\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m infos:\n",
      "File \u001b[1;32mc:\\Users\\karthik chirag\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_reader.py:289\u001b[0m, in \u001b[0;36mffmpeg_parse_infos\u001b[1;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[0;32m    287\u001b[0m         result[\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m cvsecs(match)\n\u001b[0;32m    288\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m((\u001b[39m\"\u001b[39m\u001b[39mMoviePy error: failed to read the duration of file \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mHere are the file infos returned by ffmpeg:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m)\u001b[39m%\u001b[39m(\n\u001b[0;32m    291\u001b[0m                           filename, infos))\n\u001b[0;32m    293\u001b[0m \u001b[39m# get the output line that speaks about video\u001b[39;00m\n\u001b[0;32m    294\u001b[0m lines_video \u001b[39m=\u001b[39m [l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lines \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m Video: \u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m l \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+x\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m'\u001b[39m, l)]\n",
      "\u001b[1;31mOSError\u001b[0m: MoviePy error: failed to read the duration of file C:/Users/karthik chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/DEMO/Examples/01-01-02-02-02-02-24.mp4Zone.Identifier.\nHere are the file infos returned by ffmpeg:\n\nffmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\r\n  built with gcc 9.2.1 (GCC) 20200122\r\n  configuration: --enable-gpl --enable-version3 --enable-sdl2 --enable-fontconfig --enable-gnutls --enable-iconv --enable-libass --enable-libdav1d --enable-libbluray --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libtheora --enable-libtwolame --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libzimg --enable-lzma --enable-zlib --enable-gmp --enable-libvidstab --enable-libvorbis --enable-libvo-amrwbenc --enable-libmysofa --enable-libspeex --enable-libxvid --enable-libaom --enable-libmfx --enable-amf --enable-ffnvcodec --enable-cuvid --enable-d3d11va --enable-nvenc --enable-nvdec --enable-dxva2 --enable-avisynth --enable-libopenmpt\r\n  libavutil      56. 31.100 / 56. 31.100\r\n  libavcodec     58. 54.100 / 58. 54.100\r\n  libavformat    58. 29.100 / 58. 29.100\r\n  libavdevice    58.  8.100 / 58.  8.100\r\n  libavfilter     7. 57.100 /  7. 57.100\r\n  libswscale      5.  5.100 /  5.  5.100\r\n  libswresample   3.  5.100 /  3.  5.100\r\n  libpostproc    55.  5.100 / 55.  5.100\r\n[lrc @ 0000016446d89240] Format lrc detected only with low score of 5, misdetection possible!\r\nInput #0, lrc, from 'C:/Users/karthik chirag/Downloads/Emotion-Recognition_SER-FER_RAVDESS/Emotion-Recognition_SER-FER_RAVDESS-main/DEMO/Examples/01-01-02-02-02-02-24.mp4Zone.Identifier':\r\n  Duration: N/A, bitrate: N/A\r\n    Stream #0:0: Subtitle: text\r\nAt least one output file must be specified\r\n"
     ]
    }
   ],
   "source": [
    "audiofile = mp.AudioFileClip(filename).set_fps(sr)\n",
    "audio = audiofile.to_soundarray()\n",
    "audio = audio[int(sr/2):int(sr/2 + sr*3)]\n",
    "audio = np.array([elem[0] for elem in audio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128, 282, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel = librosa.power_to_db(librosa.feature.melspectrogram(y=audio, sr=48000, n_fft=1024, n_mels=128, fmin=50, fmax=24000))\n",
    "\n",
    "\n",
    "scaler = load(parameters_path)\n",
    "mel = scaler.transform(mel)\n",
    "\n",
    "mel = np.expand_dims(mel, axis = 2)\n",
    "mel = np.expand_dims(mel, axis = 0)\n",
    "mel.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = os.listdir(models_video_path)\n",
    "\n",
    "acc = [float(model.split('[')[1].split(']')[0]) for model in models_list]\n",
    "idx = acc.index(max(acc))                                                       # index of best model\n",
    "\n",
    "model_video = keras.models.load_model(models_video_path + models_list[idx])\n",
    "# model_video.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = os.listdir(models_audio_path)\n",
    "model_audio = keras.models.load_model(models_audio_path + models_list[0])\n",
    "#model_audio.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002EA53EBBF60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 308ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.9035088e-04, 2.6006729e-03, 5.5903613e-05, 1.1917308e-04,\n",
       "       9.3914616e-01, 6.4361470e-06, 5.7681356e-02], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_video.predict(frames)\n",
    "pred_video = np.mean(pred, axis=0)\n",
    "pred_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002EA4661D440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 113ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.4040872e-02, 5.5525906e-04, 7.1970981e-01, 1.8299115e-01,\n",
       "       3.4310367e-02, 3.6364529e-02, 2.0279970e-03], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model_audio.predict(mel)\n",
    "pred_audio = np.mean(pred, axis=0)\n",
    "pred_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_global = pred_video + pred_audio # mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video prediction:\t happy\n",
      "Audio prediction:\t disgust\n",
      "Global prediction:\t happy\n",
      "Ground truth:\t\t happy\n"
     ]
    }
   ],
   "source": [
    "print('Video prediction:\\t', emotions[pred_video.argmax()])\n",
    "print('Audio prediction:\\t', emotions[pred_audio.argmax()])\n",
    "print('Global prediction:\\t', emotions[pred_global.argmax()])\n",
    "\n",
    "print('Ground truth:\\t\\t', emotions[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Results\n",
    "root = tk.Tk()\n",
    "\n",
    "# root window title and dimension\n",
    "root.title(\"Results predictions\")\n",
    "\n",
    "canvas1 = tk.Canvas(root, width=400, height=300, relief='raised')\n",
    "canvas1.pack()\n",
    "\n",
    "label1 = tk.Label(root, text=f'Video prediction:\\t{emotions[pred_video.argmax()]}')\n",
    "label2 = tk.Label(root, text=f'Audio prediction:\\t{emotions[pred_audio.argmax()]}')\n",
    "label3 = tk.Label(root, text=f'Global prediction:\\t{emotions[pred_global.argmax()]}')\n",
    "label4 = tk.Label(root, text=f'Ground truth:\\t{emotions[label]}')\n",
    "\n",
    "label1.config(font=('helvetica', 14))\n",
    "label2.config(font=('helvetica', 14))\n",
    "label3.config(font=('helvetica', 14))\n",
    "label4.config(font=('helvetica', 14), fg='gray')\n",
    "\n",
    "canvas1.create_window(20, 25, window=label1, anchor='w')\n",
    "canvas1.create_window(20, 50, window=label2, anchor='w')\n",
    "canvas1.create_window(20, 75, window=label3, anchor='w')\n",
    "canvas1.create_window(20, 120, window=label4, anchor='w')\n",
    "\n",
    "button1 = tk.Button(text='Close', command=lambda: root.destroy(), font=('helvetica', 12, 'bold'))\n",
    "canvas1.create_window(200, 200, window=button1)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
